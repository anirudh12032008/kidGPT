# -*- coding: utf-8 -*-
"""kidGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_YwS7DuLnZV3SfhgFbfgzJYTYUvC9zT

# Loading Dataset
"""

import pandas as pd
import re
from collections import Counter
from torch.utils.data import Dataset

splits = {
    'train': 'data/train-00000-of-00001-aea567a5783be827.parquet',
    'valid': 'data/valid-00000-of-00001-0177e6dbdee45eef.parquet',
    'test': 'data/test-00000-of-00001-0b94257b623049e5.parquet'
}
df_train = pd.read_parquet("hf://datasets/deven367/babylm-10M-children-stories/" + splits["train"])
df_valid = pd.read_parquet("hf://datasets/deven367/babylm-10M-children-stories/" + splits["valid"])
df_test = pd.read_parquet("hf://datasets/deven367/babylm-10M-children-stories/" + splits["test"])

print(df_train.head())

"""## Vocabulary"""

def tokenize(text):
    text = text.lower()
    tokens = re.findall(r'\b\w+\b', text)
    return tokens

counter = Counter()
for text in df_train["text"]:
    counter.update(tokenize(text))

vocab = {word: idx for idx, (word, _) in enumerate(counter.most_common(), start=1)}
vocab['<PAD>'] = 0
inverse_vocab = {idx: word for word, idx in vocab.items()}


def text_to_indices(text, vocab):
    tokens = tokenize(text)
    return [vocab[token] for token in tokens if token in vocab]

df_train["input_ids"] = df_train["text"].apply(lambda x: text_to_indices(x, vocab))
df_valid["input_ids"] = df_valid["text"].apply(lambda x: text_to_indices(x, vocab))

df_train.head()

"""## Dataset creation"""

import torch
from torch.utils.data import Dataset

class TextDataset(Dataset):
    def __init__(self, dataframe, max_length=128):
        self.data = dataframe
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        input_ids = self.data.iloc[idx]["input_ids"]
        if len(input_ids) > self.max_length:
            input_ids = input_ids[:self.max_length]
        else:
            input_ids = input_ids + [vocab['<PAD>']] * (self.max_length - len(input_ids))

        return torch.tensor(input_ids)

train_dataset = TextDataset(df_train, max_length=128)
valid_dataset = TextDataset(df_valid, max_length=128)

"""# Model Designing"""

import torch
import math
import torch.nn as nn
import torch.nn.functional as F


class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        self.attention = nn.MultiheadAttention(embed_size, heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)

        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention_output, _ = self.attention(query, key, value, attn_mask=mask)
        x = self.dropout(self.norm1(attention_output + query))
        forward_output = self.feed_forward(x)
        out = self.dropout(self.norm2(forward_output + x))
        return out

"""## Language Model

"""

class LanguageModel(nn.Module):
    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):
        super(LanguageModel, self).__init__()
        self.device = device
        self.embed_size = embed_size

        self.word_embedding = nn.Embedding(vocab_size, embed_size)
        self.position_embedding = PositionalEncoding(embed_size, max_len=max_length)

        self.layers = nn.ModuleList(
            [
                TransformerBlock(
                    embed_size,
                    heads,
                    dropout=dropout,
                    forward_expansion=forward_expansion
                )
                for _ in range(num_layers)
            ]
        )
        self.fc_out = nn.Linear(embed_size, vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        out = self.word_embedding(x)
        out = self.position_embedding(out)
        for layer in self.layers:
            out = layer(out, out, out, mask)
        out = self.fc_out(out)
        return out
def generate_text(model, vocab, inverse_vocab, prompt, max_length=50, device='cpu'):
    model.eval()
    input_ids = [vocab.get(token, vocab['<PAD>']) for token in tokenize(prompt)]
    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)

    generated = input_tensor
    for _ in range(max_length):
        with torch.no_grad():
            outputs = model(generated, mask=None)
            next_token_logits = outputs[:, -1, :]
            next_token_probs = F.softmax(next_token_logits, dim=-1)
            next_token_id = torch.multinomial(next_token_probs, num_samples=1).item()
            generated = torch.cat((generated, torch.tensor([[next_token_id]]).to(device)), dim=1)

            if next_token_id == vocab['<PAD>']:
                break

    generated_text = [inverse_vocab.get(idx, '<UNK>') for idx in generated.squeeze().tolist()]
    return ' '.join(generated_text)

"""# Training"""

from torch.optim import AdamW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
vocab_size = len(vocab)
embed_size = 256
num_layers = 4
heads = 8
forward_expansion = 4
dropout = 0.1
max_length = 128

model = LanguageModel(
    vocab_size=vocab_size,
    embed_size=embed_size,
    num_layers=num_layers,
    heads=heads,
    device=device,
    forward_expansion=forward_expansion,
    dropout=dropout,
    max_length=max_length
).to(device)

optimizer = AdamW(model.parameters(), lr=5e-4)
loss_fn = nn.CrossEntropyLoss()

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)

for epoch in range(20):
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch.to(device)

        output = model(input_ids[:, :-1], mask=None)
        loss = loss_fn(output.view(-1, vocab_size), input_ids[:, 1:].reshape(-1))

        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}")

"""# Time for Graphs"""

import matplotlib.pyplot as plt

train_losses = []

for epoch in range(5):
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch.to(device)

        output = model(input_ids[:, :-1], mask=None)
        loss = loss_fn(output.view(-1, vocab_size), input_ids[:, 1:].reshape(-1))

        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)

    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(train_losses) + 1), train_losses, marker='o', color='b', label='Training Loss')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

"""#Testing"""



while True:
  prompt = input("Prompt")
  if prompt == "q":
    break
  generated_text = generate_text(model, vocab, inverse_vocab, prompt, max_length=50, device=device)
  print("Generated text: ", generated_text)

"""#Saving"""

torch.save(model.state_dict(), "kidGPT.pth")